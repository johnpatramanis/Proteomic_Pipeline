\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{spverbatim}
\usepackage[english]{babel}
\usepackage[noend]{algpseudocode}
\usepackage{xparse}
\clearpage
\usepackage{caption}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\urlstyle{same}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{authblk}
\usepackage{natbib}
\usepackage{graphicx}


\setcounter{secnumdepth}{0}
\DeclareCaptionJustification{raggedright}{\raggedright}
\setmarginsrb{1.8 cm}{2.5 cm}{1.8 cm}{2.5 cm}{1 cm}{1 cm}{1.5 cm}{1.5 cm}


\graphicspath{{./graphics/}}
\selectlanguage{english}
\begin{document}

\title{PaleoProPhyler, Supplementary Material}
\author[1]{Ioannis Patramanis}
\author[3]{Jazmin Ramos Madrigal}
\author[2]{Enrico Cappellini}
\author[1,2]{Fernando Racimo}

\affil[1]{\footnotesize Section for Molecular Ecology and Evolution
, Globe Institute, University of Copenhagen}
\affil[2]{\footnotesize GeoGenetics Section, Globe Institute, University of Copenhagen }
\affil[3]{\footnotesize Center for Evolutionary Hologenomics, Globe Institute, University of Copenhagen  }


\date{\today}
\maketitle












\selectlanguage{english}







\section{\huge Detailed Overview of Pipelines}
In the next few pages you will find a more in depth, step by step, overview of what is running in each module of the pipelines.

\vspace{1cm}

\subsection{\huge Module 1:}
Step by step scripts that run:

\begin{enumerate}

  \item Python3 Script to fetch Ensembl Gene ID for each Protein - Organism Combination (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Initialization/Python_Scripts/Get_Gene_ID.py}{Link to Script})
  \item Python3 Script to fetch Transcript ID  ( Ensembl-Canonical and Optional Alternative Isoforms ) for each Gene ID (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Initialization/Python_Scripts/Search_Ensembl.py}{Link to Script})
  \item Python3 Script to fetch Protein Fasta Sequence for each Transcript ID (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Initialization/Python_Scripts/Get_Prot_Sequence_Ensembl.py}{Link to Script})
  \item Python3 Script to fetch Protein Exon and Intron information for each Transcript ID ( and create a table for them) (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Initialization/Python_Scripts/Get_Exon_Intron_Table.py}{Link to Script})
  \item Python3 Script to fetch Gene location (corrected for Reference version) for each Transcript ID (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Initialization/Python_Scripts/Get_Assembly_Location.py}{Link to Script})
  \item Python3 Script to combine all FASTA Sequences into datasets ( per Protein and per Organism ) (Part of main Snakemake Script)
\end{enumerate}

\vspace{1cm}

{\large Input of Module 1:}
\begin{itemize}
\item a TXT file with a list of coded protein names, one per line e.g.:
\begin{itemize}
\item[] AMELX
\item[] ENAM
\item[] AMELY
\end{itemize}

\item Alternatively this list can also contain the names of specific isoforms, or the word ALL to bring all known isoforms e.g.:

\begin{itemize}
\item[] AMELX::AMELX-201
\item[] ENAM::ENAM-202
\item[] AMELY::ALL
\end{itemize}


\item a TXT file with a list of scientific organism names e.g.:
\begin{itemize}
\item[] homo sapiens
\item[] pan troglodytes
\item[] gorilla gorilla
\end{itemize}


\item Alternatively this second list can also contain the names of specific reference versions e.g.:
\begin{itemize}
\item[] homo sapiens \quad GRCh37
\item[] pan troglodytes \quad  CHIMP2.1.4
\item[] gorilla gorilla \quad gorGor3.1
\end{itemize}

\end{itemize}
	 

\vspace{2cm}




	
	
	
	
	
	
	
	
	
	
	




\subsection{\huge Module 2}

\vspace{1.2cm}

{\large This module has 3 alternative input file options:}
\vspace{0.5cm}

\begin{itemize}

\item {\large CRAM input:}

\begin{itemize}	
\item Transformed into BAM file using the command \verb| "samtools view -b" |
\item Follows BAM input path
\end{itemize}

\item {\large BAM input:}
\begin{itemize}	

\item Headers are renamed, e.g. from ‘Chr1’ to just ‘1’, using the commands \verb| ‘samtools view’ | and \verb| ‘samtools reheader’ |
\item Renamed BAM file is index with \verb| ‘samtools index -b’ |
\item BAM file is split into chromosome BAM files using \verb| ‘samtools view -b’ |
\item Chromosome BAM files are re-indexed using \verb| ‘samtools index -b’ |
\item BAM files are transformed to FASTA file using \begin{spverbatim} "angsd -minQ 30 -minMapQ 30 -doFasta 2 -doCounts 1 \end{spverbatim}
\verb| -basesPerLine 60" |
\end{itemize}

\item {\large VCF input:}
\begin{itemize}	
\item A reference genome in FASTA format needs to be provided and placed in the \begin{spverbatim} 'Dataset_Construction/Reference/' \end{spverbatim} folder
\item The provided reference genome is renamed to the same standard as the BAM files (‘Chr1’ to just ‘1’)
\item The VCF file is renamed as well, using \verb| 'bcftools view' | , \verb| 'bgzip -c' | and \verb| 'tabix -C -p vcf' |
\item VCF file is converted to FASTA using a combination of \verb| ‘samtools faidx' | and
\begin{spverbatim}
'bcftools consensus --missing ? -s'
\end{spverbatim}


\end{itemize}
\end{itemize}

\vspace{1.2cm}


{\large All different inputs are now in the same format and will follow the same workflow:}
\vspace{1.2cm}

\begin{enumerate}	
\item Custom R script that uses Exon / Intron Locations to splice DNA FASTA (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Construction/R%20scripts/Rscript2.r}{Link to Script} )
\item Blast Reference Protein onto spliced DNA FASTA with \verb| ‘makeblastdb -dbtype nucl’ | and\begin{spverbatim}  ‘tblastn -seg no -ungapped -comp_based_stats F  -outfmt 5’\end{spverbatim}
\item Custom Python3 script to extract blasted / translated protein and output it in FASTA format (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Construction/Python%20Scripts/BLAST_EXTRACTOR.py}{Link to Script}).
\item Shell commands to merge together individual proteins into larger datasets (per Protein / per Individual
\end{enumerate}


\vspace{2cm}



















\subsection{\huge Module 3:}

\subsubsection{Input}

The main input of this module is a FASTA file containing both the ancient sequences to be analysed as well as the full reference data set.

\begin{itemize}
\item All proteins for all individuals, the format of fasta sequence labels should be: \verb| >SampleName_ProteinName|
\item User must also provide the names of the ancient samples in the analysis. Proteins that are not found in any of the ancient samples but exist inside the input fasta file, will not be included in the analysis.

\item Optional - Masking: If the user want to mask a modern sample with the missingness of an ancient one they need to provide a file named 'MASKED' in the module's main directory. This file should contain two columns and as many rows as necessary. Each row should contain the names of two samples, first the name of a modern sample to be masked and second, separated by a whitespace, the name of an ancient sample. The missing positions of each ancient sample will be masked on top of the same positions of the modern sample, ‘masking’ it as an ancient sample. 

\end{itemize}

\subsubsection{Workflow}

\begin{enumerate}

\item The initial FASTA is split into protein-specific FASTAs with a custom R script (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Analysis/Rscripts/Rscript1.r}{Link to Script}).

\item Each protein-specific dataset is aligned using Mafft:
\item Modern and Ancient samples are first separated.
\item Modern samples are aligned with \begin{spverbatim} ‘mafft --ep 0 --op 0.5 --lop -0.5 --genafpair --maxiterate 20000 --bl 80 --fmodel‘ \end{spverbatim}
\item Ancient samples are merged and aligned with modern ones with the ‘mafft-einsi  --addlong’ option.
\item Aligned dataset is trimmed of completely missing positions using  ‘trimal -noallgaps’

\item A custom R script generates a small table of  statistics for each ancient sample in the dataset after correcting for I/L positions (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Analysis/Rscripts/Rscript2.r}{Link to Script}). 

\item A custom R script concatenates protein-specific FASTAS into Concatenated FASTA. User has the option to filter proteins under certain coverage. A \verb| ‘Partition_Helper’| file is also generated, which contains start and stop positions of each protein and can be utilised by NEXUS format phylogenetic software (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Analysis/Rscripts/Rscript3.r}{Link to Script}).

\item A custom R script is used to convert FASTA files to PHYLIP format (\href{https://github.com/johnpatramanis/Proteomic_Pipeline/blob/main/Dataset_Analysis/Rscripts/Rscript4.r}{Link to Script}).

\item Multithreaded version of PhyML is run on each separate protein-specific data set using:
\begin{spverbatim}
mpirun -n threads phyml-mpi -i dataset.phy -d aa -b 100 -m JTT -a e -s BEST -v e -o tlr -f m --rand_start --n_rand_starts 4 --r_seed (random_seed_number) --print_site_lnl --print_trace --no_memory_check 
\end{spverbatim}

\item Concatenated data set is converted to NEXUS format using ‘seqmagick convert --output-format nexus --alphabet protein’
, with minor bash command line fixes to ensure proper formatting.

\item Masking: Optional step of 'masking' a modern sample with the missingness of an ancient sample, takes palce at this step



\item Multithreaded version of MrBayes is run on the concatenated NEXUS file using the MrBayes commands:

\begin{spverbatim}
prset aamodelpr = mixed;
mcmc nchains = (number of cores)/4 nruns=(number of cores)/8 ngen = 10000000 samplefreq=100 printfreq=100 diagnfreq=1000;
sumt relburnin = yes burninfrac = 0.25;
sump;
\end{spverbatim}
and adding the input of the \verb| ‘Partition_Helper’ | file and finally running the file by
\verb| "mpirun -np (number of cores/4) mb-mpi“ |


\end{enumerate}


















\clearpage

\section{\Large Requirements for running the pipeline}

The only true requirement for running any of the 3 modules, is having a Linux machine with Conda installed.
All of the required software and packages are downloaded and installed through conda using the provided conda environments. Bellow you can find the full list of software and package used by the pipeline.

\subsection{OS Requirements:}
\begin{itemize}	
\item Linux
\end{itemize}


\subsection{List of Software and version used by the pipeline:}
\begin{itemize}	
\item Snakemake v7.3.6 \cite{molder2021sustainable}
\item Conda v22.9.0 \cite{anaconda}
\item Samtools v1.15 \cite{li2009sequence}
\item BCFtools v1.15 \cite{li2011statistical}
\item Blast v2.12.0+ \cite{national2008blast}
\item Angsd v0.937 \cite{korneliussen_angsd:_2014}
\item Mafft v7.490 \cite{katoh2013mafft,long2016determination}
\item Trimal v1.4.rev15 \cite{capella2009trimal}
\item Mpirun v4.1.1 \cite{mpi40}
\item PhyML v3.3.20200621 \cite{guindon2010new}
\item MrBayes v3.2.7 \cite{huelsenbeck2001mrbayes}
\item Seqmagick v0.8.4 \url{https://github.com/fhcrc/seqmagick}
\item R v4.1.0 \cite{team2013r}
\item Python 3 v3.9.6 \cite{10.5555/1593511}

\end{itemize}

\subsection{R packages}
\begin{itemize}	
\item Bioconductor - ShortRead v1.50.0 \cite{morgan2009shortread}
\item Phyclust  v0.1.30 \cite{chen2011overlapping}
\item Stringr v1.4.0 \cite{STRINGRR}
\end{itemize}


\subsection{Python packages}

\begin{itemize}	
\item Biopython v1.79 \cite{cock2009biopython}
\item OS package \cite{10.5555/1593511}
\item Sys package \cite{10.5555/1593511}
\item Requests package v2.26.0 \cite{chandra2015python}
\item RE package v2.2.1\cite{van1995python}
\end{itemize}


\clearpage















\section{\Large Palaeo proteomic hominid reference dataset}

\subsection{\large Choosing and preparing the list of proteins.}

We selected 6 publications cataloging proteins identified in either teeth or bone tissue\cite{castiblanco2015identification,alves2011unraveling,acil2005detection,salmon2016global,jagr2012comprehensive,park2009proteomics}. From these publications we compiled a list of 1696 unique protein names, which are provided in this file,in the main repository:
\verb|“Reference_Protein_List.txt”|
We modified this list and used it as an input for Module 1 of the pipeline. From these 1696 proteins, 153 could not be matched to an Ensembl Gene ID. This left a total of 1543 proteins that were successfully translated. 

\vspace{1.2}

\subsection{\large Choosing and preparing samples for translation.}

Samples were chosen and used from the following publications:
\begin{itemize}
\item (1) 1KG high coverage\cite{byrskahigh}. 
\item (2) Great ape genomes project\cite{prado2013great}. 
\item (3) Morphometric, Behavioral, and Genomic Evidence for a New Orangutan Species\cite{nater2017morphometric}. 
\item (4) A high-coverage Neandertal genome from Vindija Cave in Croatia\cite{prufer2017high}. 
\item (5) A high-coverage Neandertal genome from Chagyrskaya Cave\cite{mafessoni2020high}. 
\end{itemize}

\vspace{1.2}

\subsection{\large Choosing individuals for the data set:}

For publications 2,3 and 5 all available samples were used.
For publication 1, only a maximum of 20 individuals from each population were used, resulting in a final X number of individuals.
For publication 4 , the individual named ‘Mezmaiskaya’ was removed from the data set due to a high amount of predicted unique SAPs. We believe that due to the low coverage of the individual, a high number of variants might be miss-called.



\subsection{\large Reference Genomes:}

We chose to use the human reference genome as the basis for our translations, due to its higher level of annotation. For this purpose, all individuals were mapped onto either GRCh37\cite{church2011modernizing} or GRCh38\cite{schneider2017evaluation}. Individuals from datasets 1,4 and 5 were already mapped onto a human reference genome. For datasets 2 and 3, raw fastq files were downloaded and mapped onto the GRCh38  human reference. The mapping workflow is provided in the form of a snakemake python script, along with a conda environment containing all software necessary to run the script.

The re-mapped bam files are available at: https://zenodo.org/..data-deposit-link

\subsection{\large Final execution:}
Both BAM files and VCF files were then used as input for Module 2, as exemplified by the Tutorial.
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}